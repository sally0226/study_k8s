# 4장 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

파드를 안정적으로 사용할 수 있는 여러 방법

- liveness prove
- replication controller
- replica set
- demon set
- job
- cron job

## 4.1 pod를 안정적으로 유지하기

쿠버네티스의 장점

- k8s에 컨테이너 목록을 제공하면, 해당 컨테이너를 클러스터 어딘가에서 계속 실행되게 할 수 있다

pod가 node에 스케줄링된 이후에 

컨테이너에 crash가 발생하면, Kubelet이 컨테이너를 재시작한다. 

### Liveness Probe (라이브니스 프로브)

- liveness probe는 애플리케이션이 무한 루프나 교착 상태에 빠지는 등 crash가 발생하지는 않았지만 정상적인 응답을 할 수 없는 상태를 체크한다.

- pod의 스펙(specification)에 각 컨테이너의 liveness probe를 지정한다.
- k8s는 주기적으로 probe를 실행하고, 실패할 경우 컨테이너를 재시작한다.

세 가지 종류

1. HTTP GET probe
    - 특정 ip, port, 경로에 GET 요청을 수행
    - 에러를 response하거나 응답하지 않는 경우 → 컨테이너 재시작
2. TCP socket probe
    - 컨테이너의 특정 port에 TCP 연결 시도
    - 연결에 실패 → 컨테이너 재시작
3. Exec probe
    - 컨테이너 내의 임의의 명령을 실행 후 명령의 종료 상태 코드 체크
    - 상태코드가 0이면 성공
    - 0이 아닌경우 → 컨테이너 재시작

### HTTP 기반 liveness probe 생성

```bash
# pod 생성 명령어 (3장 내용)
kubectl create -f {file_name}.yaml
```

```yaml
# kubia_liveness_probe.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy # 문제가 있는 이미지
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```

```yaml
kubectl get po kubia-liveness
```

위 명령어로 pod 상태를 확인하면 RESTARTS 횟수가 점점 늘어나는 것을 볼 수 있다

**pod 재시작 후 describe로 확인**

```bash
> kubectl describe po kubia-liveness                                             ✔  nginx-1-cluster ⎈  17:15:51  
Name:         kubia-liveness
Namespace:    default
Priority:     0
Node:         gke-nginx-1-cluster-default-pool-cde5cbdb-2jvh/10.128.15.227
Start Time:   Wed, 12 Jul 2023 17:13:45 +0900
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.68.0.9
IPs:
  IP:  10.68.0.9
Containers:
  kubia:
    Container ID:   containerd://902721d2b6d19e828bebfd295c80fa2df59b63b364eb30da7823fcfe551807be
    Image:          luksa/kubia-unhealthy
    Image ID:       docker.io/luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 12 Jul 2023 17:15:46 +0900
    Last State:     Terminated # 컨테이너가 재시작된 이유 확인 가능
      Reason:       Error
      Exit Code:    137 # 특수한 종료 코드
      Started:      Wed, 12 Jul 2023 17:14:05 +0900
      Finished:     Wed, 12 Jul 2023 17:15:46 +0900
    Ready:          True
    Restart Count:  1
    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 # 라이브니스 프로브 추가 속성 확인 가능
    Environment:    <none>

..............

Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  5m31s                 default-scheduler  Successfully assigned default/kubia-liveness to gke-nginx-1-cluster-default-pool-cde5cbdb-2jvh
  Normal   Pulled     5m11s                 kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 18.378966681s (18.379015398s including waiting)
  Normal   Pulled     3m30s                 kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 361.23599ms (361.252644ms including waiting)
  Normal   Created    100s (x3 over 5m11s)  kubelet            Created container kubia
  Normal   Started    100s (x3 over 5m11s)  kubelet            Started container kubia
  Normal   Pulling    100s (x3 over 5m30s)  kubelet            Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled     100s                  kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 370.658796ms (370.665028ms including waiting)
  Warning  Unhealthy  20s (x9 over 4m20s)   kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal   Killing    20s (x3 over 4m)      kubelet            Container kubia failed liveness probe, will be restarted
```

- Exit Code `137`
    - 프로세스가 **외부 신호에 의해 종료**되었음을 나타냄
    - 128 + x
        - x : 프로세스에 전송된 시그널 번호
        - 여기서 x = 9 (SIGKILL) 프로세스가 강제 종료되었음을 의미
- Events
    - 컨테이너가 종료된 이유를 보여줌
- Liveness
    
    ```bash
    http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 
    ```
    
    - liveness probe의 추가 속성 확인 가능
    - delays = 0s
        - 컨테이너 시작 후 바로 프로브 시작
        - tip ) 애플리케이션 시작시간을 고려해 delay를 설정해야 한다
    - timeout = 1s
        - 컨테이너가 1초 안에 응답해야 함
    - period=10s
        - 컨테이너는 10초마다 프로브를 수행
    - failure=3
        - 프로브가 3번 연속 실패하면 컨테이너 재시작
    - 이러한 속성들은 yaml파일내부에 지정할 수 있다

- 크래시된 컨테이너의 로그 얻기
    
    `--previous` 옵션 사용
    
    ```yaml
    kubectl logs podname --previous
    ```
    

### 효과적인 liveness probe 생성

운영 환경에서 실행중인 pod에 liveness probe를 정의하지 않으면, k8s가 애플리케이션이 살아있는지 알 수 있는 방법이 없다. (필수다!)

- 더 나은 probe
    - 위에서 만든 것 처럼 간단한 프로브로도 충분할 수 있지만,
    - 더 나은 liveness probefmf dnlgo 특정 URL경로 (ex /health)에 요청하도록 구성해, 애플리케이션 내에 실행중인 모든 구성요소를 체크하도록 구성할 수 있다.
- probe를 가볍게 유지
    - 기본적으로 1초내에 완료되어야 한다
    - 너무 오래걸리면 컨테이너의 속도 저하 원인이 된다.
- probe에 재시도 루프 금지

### liveness probe 요약

- 컨테이너에 크래시 발생 or liveness probe 실패 → k8s의 kubelet에서 컨테이너 재실행
- 노드 자체에 크래시가 발생한 경우 → 컨트롤 플레인의 몫
- kubelet은 노드에서 실행되기 때문에, 노드가 고장나면 아무것도 할 수 없다.

## 4.2 레플리케이션 컨트롤러 (rc)

- **파드**가 항상 실행되도록 보장
- 어떤 이유든 파드가 사라지면, 사라진 파드를 감지해 교체 파드 생성

<img width="691" alt="Untitled" src="https://github.com/sally0226/study_k8s/assets/43634786/b2ddc964-2fa5-41ce-83c9-5ced6286857c">

- rc의 동작
    - 실행중인 파드 목록을 지속적으로 모니터링
    - 특정 유형의 실제 파드 수가 의도한 바와 일치하는지 확인
    - 파드가 적게 실행되는 경우 파드 템플릿에서 새 복제본 생성
    - 파드가 많이 실행되는 경우 초과 복제본 제거
    

<img width="603" alt="Untitled 1" src="https://github.com/sally0226/study_k8s/assets/43634786/34584148-9d0e-43ba-8d86-9a7baa349b67">


- rc의 세 가지 필수 요소
    - label selector : rc 범위에 있는 파드 결정
    - replica count : 실행할 파드의 desired count 지정
        - 변경시 기존 파드에 영향 미침 (초과시 삭제됨)
    - pod template : 새로운 파드 레플리카 만들 때 사용 (붕어빵 틀)

<img width="298" alt="Untitled 2" src="https://github.com/sally0226/study_k8s/assets/43634786/02a4c180-8397-4b8f-9e47-7ee8f7da2584">


- rc를 사용하면 얻는 이점
    - 파드(또는 여러 파드의 복제본)이 항상 실행되도록 보장
    - node에 장애가 발생하면, 해당 노드에 실행중인 모든 파드에 대한 교체 복제본 생성
    - pod 수평확장이 쉬워짐
- rc 생성
    
    ```yaml
    # kubia_rc.yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: kubia
    spec:
      replicas: 3
      selector: # 파드 셀렉터로 rc가 관리할 pod 선택
        app: kubia 
      template: # 새 파드에 사용될 파드 템플릿
        metadata:
          labels:
            app: kubia
        spec:
          containers:
          - name: kubia
            image: luksa/kubia
            ports:
            - containerPort: 8080
    ```
    
    - template의 pod label ≠ rc의 label selector 인 경우 rc 생성되지 않는다
        - 만약 이런 설정의 rc가 생성되면 pod가 무한 생성될 수 있기 때문이다
    - tip) rc의 label selector를 지정하지 않으면, template의 pod label로 자동 설정된다
- rc 동작 확인
    - pod 목록을 조회하면 위에서 만든 rc가 pod3개를 만들어서 관리하고 있는걸 확인할 수 있다
    - 의도적으로 하나를 지우면 갯수맞춰서 새로 생성한다
        - pod 삭제에 대응하는 것이 아니라 결과적 상태(부족한 파드 수)에 대응하는 것
        - pod 삭제는 트리거 역할을 할 뿐
            
            <img width="656" alt="Untitled 3" src="https://github.com/sally0226/study_k8s/assets/43634786/c0acf717-ef81-48dd-9984-375aee9fa26f">

            
        - 노드에 장애가 발생한 경우에도 마찬가지로 해당 노드에 스케줄링된 파드는 상태가 알수없음으로 변경되고 rc에 의해 재시작된다
- rc 범위 안팎으로 파드 이동하기
    - rc는 label selector와 일치하는 파드만을 관리
    - 해당 rc에 의해 생성되었는지 여부는 중요하지 않음
    - rc의 label selector설정에 따라 pod를 관리하는 rc가 달라질 수 있다
    - pod가 rc에 묶여있지는 않지만, pod의 metadata.ownerReferences 필드에서 pod가 속한 rc를 참조함

- pod의 label수정에 따른 변화
    - rc가 관리하는 pod에 레이블 추가 → 변동 X
    - rc가 관리하는 pod의 레이블 변경 → 더 이상 rc의 label selector와 일치하지 않는경우 관리대상에서 제외됨
        - replica count를 맞추기 위해 새로운 pod를 생성한다
    
    ```yaml
    kubectl label pod {pod이름} type=special
    kubectl get pods --show-labels # pod에 type=special label이 추가된 것을 확인
    kubectl label pod {pod이름} app=foo --overwrite # 기존 레이블 변경을 위해 overwrite 옵션 필요
    # app=kubia -> app=foo로 overwrite되어 rc의 관리범위에서 벗어남 -> rc가 새 pod(app=kubia) 생성 
    kubectl label pods -L app # 레이블을 표시하기 위한 옵션 
    ```
    
    <img width="924" alt="Untitled 4" src="https://github.com/sally0226/study_k8s/assets/43634786/5142f53e-62f0-4553-b59e-ace802e6d1ee">

    
    - 특정 pod에 작업이 필요한 경우, rc 범위에서 제거하면 작업이 수월하다
        - pod를 rc 범위 밖으로 빼내어 새 pod로 대체되도록 한 다음 디버그 혹은 원하는 작업을 할 수 있다.
- pod template 변경
    - 붕어빵 틀을 변경하는 것과 같다.
    
    <img width="858" alt="Untitled 5" src="https://github.com/sally0226/study_k8s/assets/43634786/2935f209-b590-4011-8553-31b309b5a889">

    
    ```yaml
    kubectl edit rc kubia
    # yaml 수정하고 저장하면 된다
    # 수정한 직후에는 변화가 없고
    # 후에 생성된 새 pod는 새 template의 속성을 가진다
    ```
    
    ```bash
    # ~/.zshrc 등 터미널 설정에 상응하는 파일에 넣으면 됨 
    export KUBE_EDITOR = "/usr/bin/nano" # 원하는 에디터 설정 가능 
    ```
    
- rc의 replica 수 변경하기
    1. 명령어 사용 
        
        ```bash
        kubectl scale rc kubia --replicas=10
        ```
        
    2. `kubectl edit rc kubia` 로 yaml 수정하기 (spec.replicas 값 수정)
        - 수정과 동시에 수정된 갯수로 변경됨
- rc 삭제하기

```bash
kubectl delete rc kubia --cascade=false 
# cascade 옵션을 이용해 rc만 삭제할지, rc가 관리중이던 pod도 함께 삭제할지 결정 가능
```

<img width="841" alt="Untitled 6" src="https://github.com/sally0226/study_k8s/assets/43634786/d612e4bc-c0c8-475e-8ac7-9aef4f0b540a">


## 4.3 rc 대신 레플리카셋 사용하기

rc 이후에 Replica Set이라는 유사한 리소스 도입 (이제 rc는 사용 X)

**rc와 rs의 차이**

- rs가 더 풍부한 표현식을 사용하는 pod selector 보유 (확장된 느낌)

- API version 속성
    
    `apiVersion` 이 두 가지를 지정한다
    
    - API 그룹
    - API 버전
    - ex) apps/v1
        - apps: API 그룹, API 버전 : v1

- rs 생성
    - rs를 생성해도, rs의 selector 조건을 만족하는 replica 개수만큼의 pod가 실행중일경우 아무런 변화가 일어나지 않는다. 기존에 실행중이던 pod를 관리하에 둔다
    - yaml 작성
        - selector를 rc보다 다양하게 정의 가능
        - operator 종류
            - In : 지정된 값 중 하나와 일치해야 함
            - NotIn : 지정된 값과 일치하지 않아야 함
            - Exists : 지정된 키를 가진 레이블이 포함되어야 함 (값은 상관 없음)
            - DoesNotExist : 지정된 키를 가진 레이블이 포함되지 않아야 함(값 X)
        - 여러 표현식을 지정하는 경우, 해당 표현식들이 모두 true인 pod가 rs에 의해 관리된다 (AND 연산)

## 4.4 데몬셋을 사용해 각 노드에서 정확히 한 개의 pod 실행하기

rc, rs는 k8s 클러스터 내 어딘가에 지정된 수 만큼 pod를 실행하는데 사용

클러스터의 모든 노드에, 노드당 하나의 pod만 실행되길 원하는 경우 데몬셋을 사용할 수 있다

<img width="577" alt="Untitled 7" src="https://github.com/sally0226/study_k8s/assets/43634786/839d6d54-6b64-42a2-a823-3cbb7b527947">


pod selector와 일치하는 pod 한개가 각 노드에서 수행되고 있는지 확인

- 일부 node에서만 pod를 실행하고 싶을 때
    - node-selector
- 데몬셋이 관리하는 pod는 스케줄러와 무관
- 주로 시스템 서비스를 실행하는데 사용

<img width="624" alt="Untitled 8" src="https://github.com/sally0226/study_k8s/assets/43634786/835a5c11-fcbf-484f-b06d-8026c4085584">


- ds 생성하고 노드에 해당 레이블을 추가하면 ds가 pod 생성해줌

```bash
kuectl label node {노드이름} disk=ssd
kubectl label node {노드이름} disk=hdd --overwrite
```

## 4.5 완료 가능한 단일 태스크를 수행하는 pod 실행

rc, rs, ds(데몬셋)은 전부 완료됐다고 간주되지 않는 지속적인 task를 실행

### job

컨테이너 내부에서 실행중인 프로세스가 완료되면 컨테이너를 다시 시작하지 않는 파드 실행 가능.

노드에 장애 발생한 경우, 해당 노드에 있던 job이 관리하는 pod는 rs pod와 같은 방식으로 다른 node로 다시 스케줄링 됨

작업이 완료되는 것이 중요한 임시 작업에 유용하다 (꼭 한번 실행되어야 하는 작업)

```bash
kubectl get job
```

- job에서 여러 pod 인스턴스 실행하기
    - 두 개 이상의 pod 인스턴스를 생성해 병렬 또는 순차적으로 실행하도록 구성 가능
        - job spec에 `completions` `parallelism` 설정해 수행
    - multi-complecation-batch-job.yaml
- job scaling
    - job이 실행되는 동안  `parallelism` 변경 가능
        
        ```bash
        kubectl scale job {job이름} --replicas 3 
        # 동작 안하는데..?
        ```
        
- 작업 소요 시간 제한하기
    - spec에 `activeDeadlineSeconds` 설정해서 실행시간을 제한할 수 있다
    - 이보다 오래 실행되면 실패한 것으로 간주
- 실패 전에 job을 재시도할 수 있는 횟수 : `backoffLimit` 설정 이용 (default : 6)

## 4.6 잡을 주기적으로 또는 한 번 실행되도록 스케줄링하기

job을 생성하면 즉시 해당하는 파드를 실행.

미래에 특정 시간 또는 지정된 간격으로 반복 실행시키고 싶은 경우. 

이런 작업을 cron이라 한다.

k8s에서는 CronJob 리소스를 만들어 구성함

## 4.7 요약

노드 장애 발생 시, pod를 계속 실행하고 스케줄링 하는 방법을 배웠다

- liveness probe
- replication controller
- replica set
- job
- cron job

## 추가적으로 공부하면 좋을 내용

- pod 삭제 비용
- pod 삭제 우선순위
